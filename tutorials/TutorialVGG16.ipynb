{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: extraction of VGG-16 representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import os.path as path\n",
    "from os import listdir \n",
    "from os.path import isfile, join\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change your paths here\n",
    "ROOT = '/home/ansuini/repos/IntrinsicDimDeep'\n",
    "\n",
    "os.chdir(ROOT)\n",
    "import sys\n",
    "sys.path.insert(0, ROOT)\n",
    "\n",
    "# and here\n",
    "results_folder = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N.of classes : 7\n"
     ]
    }
   ],
   "source": [
    "category_tags = ['n01882714','n02086240','n02087394','n02094433','n02100583','n02100735','n02279972', 'mix']\n",
    "n_objects = len(category_tags) - 1\n",
    "print('N.of classes : {}'.format(n_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6d75fbcad0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random generator init\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "arch = 'vgg16'\n",
    "nsamples = 500\n",
    "bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to select checkpoint layers and to determine their depths\n",
    "# (this works for AlexNet and VGG-like architectures)\n",
    "def getDepths(model):    \n",
    "    count = 0    \n",
    "    modules = []\n",
    "    names = []\n",
    "    depths = []    \n",
    "    modules.append('input')\n",
    "    names.append('input')\n",
    "    depths.append(0)    \n",
    "    \n",
    "    for i,module in enumerate(model.features):       \n",
    "        name = module.__class__.__name__\n",
    "        if 'Conv2d' in name or 'Linear' in name:\n",
    "            count += 1\n",
    "        if 'MaxPool2d' in name:\n",
    "            modules.append(module)\n",
    "            depths.append(count)\n",
    "            names.append('MaxPool2d')            \n",
    "    for i,module in enumerate(model.classifier):\n",
    "        name = module.__class__.__name__\n",
    "        if 'Linear' in name:\n",
    "            modules.append(module)    \n",
    "            count += 1\n",
    "            depths.append(count + 1)\n",
    "            names.append('Linear')                       \n",
    "    depths = np.array(depths)   \n",
    "    return modules, names, depths\n",
    "\n",
    "def getLayerDepth(layer):\n",
    "    count = 0\n",
    "    for m in layer:\n",
    "        for c in m.children():\n",
    "            name = c.__class__.__name__\n",
    "            if 'Conv' in name:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to select checkpoint layers and to determine their depths\n",
    "# (this works for ResNets architectures)\n",
    "def getResNetsDepths(model):    \n",
    "    modules = []\n",
    "    names = []\n",
    "    depths = []  \n",
    "    \n",
    "    # input\n",
    "    count = 0\n",
    "    modules.append('input')\n",
    "    names.append('input')\n",
    "    depths.append(count)           \n",
    "    # maxpooling\n",
    "    count += 1\n",
    "    modules.append(model.maxpool)\n",
    "    names.append('maxpool')\n",
    "    depths.append(count)     \n",
    "    # 1 \n",
    "    count += getLayerDepth(model.layer1)\n",
    "    modules.append(model.layer1)\n",
    "    names.append('layer1')\n",
    "    depths.append(count)         \n",
    "    # 2\n",
    "    count += getLayerDepth(model.layer2)\n",
    "    modules.append(model.layer2)\n",
    "    names.append('layer2')\n",
    "    depths.append(count)      \n",
    "    # 3\n",
    "    count += getLayerDepth(model.layer3)\n",
    "    modules.append(model.layer3)\n",
    "    names.append('layer3')\n",
    "    depths.append(count)     \n",
    "    # 4 \n",
    "    count += getLayerDepth(model.layer4)\n",
    "    modules.append(model.layer4)\n",
    "    names.append('layer4')\n",
    "    depths.append(count)      \n",
    "    # average pooling\n",
    "    count += 1\n",
    "    modules.append(model.avgpool)\n",
    "    names.append('avgpool')\n",
    "    depths.append(count)     \n",
    "    # output\n",
    "    count += 1\n",
    "    modules.append(model.fc)\n",
    "    names.append('fc')\n",
    "    depths.append(count)                      \n",
    "    depths = np.array(depths)    \n",
    "    return modules, names, depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate model and define checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating pre-trained model\n"
     ]
    }
   ],
   "source": [
    "print('Instantiating pre-trained model')\n",
    "model = vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode : False\n"
     ]
    }
   ],
   "source": [
    "# this switch to evaluation mode your network: in this way dropout and batchnorm \n",
    "# no more active and you can use the network as a 'passive' feedforward device; \n",
    "# forgetting this produces catastrophically wrong results (I know because I did it)\n",
    "model.eval()\n",
    "print('Training mode : {}'.format(model.training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules, names, depths = getDepths(model)\n",
    "print('List of layers from which to extract representations: {}.format(names) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images preprocessing methods\n",
    "\n",
    "mean_imgs = [0.485, 0.456, 0.406]\n",
    "std_imgs = [0.229, 0.224, 0.225]\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Data transformations (same as suggested by Soumith Chintala's script)\n",
    "data_transforms =  transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of representations\n",
    "\n",
    "\n",
    "This will extract for each class the representations of $\\sim 500$ images \n",
    "from all checkpoints, including input and output.\n",
    "\n",
    "It will save these representations as matrices of shape (n.images,embedding dimension).\n",
    "\n",
    "A typical filename will be n02086240_5.npy which means that this file contains the representations of class n02086240 extracted at the sixt (5+1) checkpoint layer, which is the max pooling after the last convolutional layer, as you can easily check by printing the list of names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input\n",
      "1 MaxPool2d\n",
      "2 MaxPool2d\n",
      "3 MaxPool2d\n",
      "4 MaxPool2d\n",
      "5 MaxPool2d\n",
      "6 Linear\n",
      "7 Linear\n",
      "8 Linear\n"
     ]
    }
   ],
   "source": [
    "for i,name in enumerate(names):\n",
    "    print(i,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(modules)\n",
    "embdims=[] # store the embedding dimension of the checkpoint layers (n.of units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: n01882714\n",
      "Processing class: n02086240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c83736f78c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mOut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0mOut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mhout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mhout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "for i,tag in enumerate(category_tags):\n",
    "    \n",
    "    print('Processing class: {}'.format(tag))\n",
    "    data_folder = join(ROOT, 'data', 'imagenet_training_single_objs', tag)\n",
    "    image_dataset = datasets.ImageFolder(join(data_folder), data_transforms)           \n",
    "    dataloader = torch.utils.data.DataLoader(image_dataset, \n",
    "                                             batch_size=bs, \n",
    "                                             shuffle=True, \n",
    "                                             num_workers=1)  \n",
    "\n",
    "    for l,module in enumerate(modules):    \n",
    "        for k, data in enumerate(dataloader, 0):\n",
    "            if k*bs > nsamples:\n",
    "                break\n",
    "            else:  \n",
    "                inputs, _ = data                          \n",
    "                if module == 'input':                \n",
    "                    hout = inputs                      \n",
    "                else:            \n",
    "                    hout = []\n",
    "                    def hook(module, input, output):\n",
    "                        hout.append(output)                \n",
    "                    handle = module.register_forward_hook(hook)                            \n",
    "                    out = model(inputs.to(device))\n",
    "                    del out   \n",
    "                    \n",
    "                    hout = hout[0] \n",
    "                            \n",
    "                    handle.remove()\n",
    "\n",
    "                if k == 0:\n",
    "                    Out = hout.view(inputs.shape[0], -1).cpu().data    \n",
    "                else :               \n",
    "                    Out = torch.cat((Out, hout.view(inputs.shape[0], -1).cpu().data),0) \n",
    "                hout = hout.detach().cpu()\n",
    "                del hout\n",
    "\n",
    "        Out = Out.detach().cpu()  \n",
    "        embdims.append(Out.shape[1])\n",
    "        \n",
    "        \n",
    "        np.save(join(results_folder, tag + '_' + str(l) + '.npy' ), Out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
